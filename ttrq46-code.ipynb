{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install setuptools==65.5.0 \"wheel<0.40.0\"\n",
    "!apt update\n",
    "!apt-get install python3-opengl\n",
    "!apt install xvfb -y\n",
    "!pip install 'swig'\n",
    "!pip install 'pyglet==1.5.27'\n",
    "!pip install 'gym[box2d]==0.20.0'\n",
    "!pip install 'pyvirtualdisplay==3.0'\n",
    "!pip install 'box2d'\n",
    "!pip install 'box2d-kengz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import itertools\n",
    "# import datetime\n",
    "# import json\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython import display as disp\n",
    "from torchsummary import summary\n",
    "from torch.distributions import Normal\n",
    "from torch.optim import Adam\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = Display(visible=0,size=(600,600))\n",
    "display.start()\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  https://github.com/honghaow/FORK/blob/master/TD3-FORK/TD3_FORK.py\n",
    "# based on https://arxiv.org/pdf/2010.01652\n",
    "#Actor nn\n",
    "# maps states (s) to actions (a)\n",
    "class Actor_Network(nn.Module):\n",
    "        def __init__(self, obs_dim, act_dim, seed=42, l1_size=400, l2_size=300):\n",
    "            super(Actor_Network, self).__init__()\n",
    "            self.seed = torch.manual_seed(seed)\n",
    "            self.l1 = nn.Linear(obs_dim, l1_size) #fully connected layers\n",
    "            self.l2 = nn.Linear(l1_size, l2_size)\n",
    "            self.l3 = nn.Linear(l2_size, act_dim)\n",
    "\n",
    "        def forward(self, s):\n",
    "            a = F.relu(self.l1(s))\n",
    "            a = F.relu(self.l2(a))\n",
    "            a = F.torch.tanh(self.l3(a)) \n",
    "            return a\n",
    "# Critic nn\n",
    "# (s,a) to Q values\n",
    "class Critic_Network(nn.Module):\n",
    "        def __init__(self, obs_dim, act_dim, seed=42, l1_size=400, l2_size=300):\n",
    "            super(Critic_Network, self).__init__()\n",
    "            self.seed = torch.manual_seed(seed)\n",
    "\n",
    "            self.q1_l1 = nn.Linear(obs_dim+act_dim, l1_size) #fully connected layers\n",
    "            self.q1_l2 = nn.Linear(l1_size, l2_size)\n",
    "            self.q1_l3 = nn.Linear(l2_size, 1)\n",
    "\n",
    "            self.q2_l1 = nn.Linear(obs_dim+act_dim, l1_size) #fully connected layers\n",
    "            self.q2_l2 = nn.Linear(l1_size, l2_size)\n",
    "            self.q2_l3 = nn.Linear(l2_size, 1)\n",
    "\n",
    "        def forward(self, s, a):\n",
    "            sa = torch.cat([s, a], 1)\n",
    "\n",
    "            q1 = F.relu(self.q1_l1(sa))\n",
    "            q1 = F.relu(self.q1_l2(q1))\n",
    "            q1 = self.q1_l3(q1)\n",
    "\n",
    "            q2 = F.relu(self.q2_l1(sa))\n",
    "            q2 = F.relu(self.q2_l2(q2))\n",
    "            q2 = self.q2_l3(q2)\n",
    "\n",
    "            return q1, q2\n",
    "# System nn\n",
    "# (s,a) to next state)\n",
    "class System_Network(nn.Module):\n",
    "        def __init__(self, obs_dim, act_dim, l1_size=400, l2_size=300):\n",
    "            super(System_Network, self).__init__()\n",
    "            self.l1 = nn.Linear(obs_dim + act_dim, l1_size)\n",
    "            self.l2 = nn.Linear(l1_size, l2_size)\n",
    "            self.l3 = nn.Linear(l2_size, obs_dim)\n",
    "        \n",
    "        def forward(self, s, a):\n",
    "            sa = torch.cat([s, a], 1)\n",
    "\n",
    "            ns = F.relu(self.l1(sa))\n",
    "            ns = F.relu(self.l2(ns))\n",
    "            ns = self.l3(ns)\n",
    "            return ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/ugurcanozalp/td3-sac-bipedal-walker-hardcore-v3/blob/main/noise.py\n",
    "# https://open.metu.edu.tr/handle/11511/92170\n",
    "# experimented with parameters\n",
    "class AbstractNoise:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def step_end(self):\n",
    "        pass\n",
    "    def episode_end(self):\n",
    "        pass\n",
    "    \n",
    "class GaussianNoise(AbstractNoise):\n",
    "    def __init__(self, mu, sigma, clip=None):\n",
    "        self.mu = mu # the mean of the gaussian distribution (typically set to 0 for zero-centered noise)\n",
    "        self.sigma = sigma # he standard deviation of the gaussian distribution, controlling the noise amplitude\n",
    "        self.clip = clip        #clip the generated noise within a specific range\n",
    "    def __call__(self):\n",
    "        delta = self.sigma*np.random.normal(size=self.mu.shape)\n",
    "        if self.clip is not None:\n",
    "            delta = delta.clip(-self.clip,+self.clip)\n",
    "\n",
    "        return self.mu + delta\n",
    "\n",
    "class OrnsteinUhlenbeckNoise(AbstractNoise): # temporal noise\n",
    "    def __init__(self, mu, theta = 0.15, sigma = 0.2, dt=0.02):\n",
    "        # 5.0, 0.02, 1.0 # 1.0, 0.02, 0.25 # 7.5, 0.02, 1.4 # 5.0, 0.02, 0.7\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt        \n",
    "        self.x_prev = np.zeros_like(self.mu)\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "# from https://github.com/ugurcanozalp/td3-sac-bipedal-walker-hardcore-v3/blob/main/env_wrappers.py\n",
    "# https://alexandervandekleut.github.io/gym-wrappers/\n",
    "#env = gym.make('BipedalWalker-v3')\n",
    "\n",
    "class EnvSkipWrapper(gym.Wrapper):\n",
    "    '''\n",
    "    This is custom wrapper for BipedalWalker-v3 and BipedalWalkerHardcore-v3. \n",
    "    Rewards for failure is decreased to make agent brave for exploration and \n",
    "    time frequency of dynamic is lowered by skipping two frames.\n",
    "    # time frequency is from  https://github.com/ugurcanozalp/td3-sac-bipedal-walker-hardcore-v3/blob/main/env_wrappers.py\n",
    "    # reward adjustment is from original FORK paper referenced in the cells above\n",
    "    '''\n",
    "    def __init__(self, env, skip=2):\n",
    "        super().__init__(env)\n",
    "        self._obs_buffer = deque(maxlen=skip)\n",
    "        self._skip = skip\n",
    "        \n",
    "    def step(self, action):\n",
    "        total_reward = 0\n",
    "        total_ep_reward = 0\n",
    "        c = 0\n",
    "        for i in range(self._skip):\n",
    "            c +=1\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_ep_reward += reward\n",
    "            if reward == -100:\n",
    "                reward = -5.0\n",
    "                info[\"dead\"] = True                    \n",
    "            else:\n",
    "                reward = 5 * reward\n",
    "                info[\"dead\"] = False\n",
    "\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        info[\"reward\"] = total_ep_reward/c\n",
    "        return obs, total_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "# or\n",
    "class EnvWrapper(gym.Wrapper):\n",
    "    '''\n",
    "    This is custom wrapper for BipedalWalker-v3 and BipedalWalkerHardcore-v3. \n",
    "    Rewards for failure is decreased to make agent brave for exploration and \n",
    "    time frequency of dynamic is lowered by skipping two frames.\n",
    "    # time frequency is from  https://github.com/ugurcanozalp/td3-sac-bipedal-walker-hardcore-v3/blob/main/env_wrappers.py\n",
    "    # reward adjustment is from original FORK paper referenced in the cells above\n",
    "    '''\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        \n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        # info[\"reward\"] = reward\n",
    "        # if reward == -100:\n",
    "        #     reward = -5.0\n",
    "        #     info[\"dead\"] = True                    \n",
    "        # else:\n",
    "        #     reward = 5 * reward\n",
    "        #     info[\"dead\"] = False\n",
    "        # return obs, reward, done, info\n",
    "        \n",
    "\n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, **kwargs):\n",
    "\n",
    "        self.env = env\n",
    "        self.device = kwargs.get(\"device\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "        self.seed = kwargs.get(\"seed\", 42)\n",
    "        self.load_weights = kwargs.get(\"load_weight\", False)\n",
    "        \n",
    "        self.obs_dim = env.observation_space.shape[0]\n",
    "        self.act_dim = env.action_space.shape[0]\n",
    "        self.act_upper_bound = self.env.action_space.high[0] \n",
    "        self.act_lower_bound = self.env.action_space.low[0]  \n",
    "        self.obs_upper_bound = self.env.observation_space.high[0] \n",
    "        self.obs_lower_bound = self.env.observation_space.low[0]  \n",
    "\n",
    "        self.critic = Critic_Network(self.obs_dim, self.act_dim, seed=self.seed)\n",
    "        self.critic_target = Critic_Network(self.obs_dim, self.act_dim, seed=self.seed)\n",
    "\n",
    "        self.actor = Actor_Network(self.obs_dim, self.act_dim, seed=self.seed)\n",
    "        self.actor_target = Actor_Network(self.obs_dim, self.act_dim, seed=self.seed)\n",
    "\n",
    "        self.system = System_Network(self.obs_dim, self.act_dim)\n",
    "\n",
    "        self.critic_optim = optim.Adam(self.critic.parameters(), lr=kwargs.get(\"lr_critic\", 3e-4))\n",
    "        self.actor_optim = optim.Adam(self.actor.parameters(), lr=kwargs.get(\"lr_actor\", 3e-4))\n",
    "        self.system_optim = optim.Adam(self.system.parameters(), lr=kwargs.get(\"lr_system\", 3e-4))\n",
    "\n",
    "        self.batch_size = kwargs.get(\"batch_size\", 100)\n",
    "        self.gamma = kwargs.get(\"gamma\", 0.99)\n",
    "        self.tau = kwargs.get(\"tau\", 0.02)\n",
    "        self.policy_freq = kwargs.get(\"policy_freq\", 2)\n",
    "        # hyperparameters for below are taken from https://github.com/ugurcanozalp/td3-sac-bipedal-walker-hardcore-v3/blob/main/env_wrappers.py and original FORK paper - specifically noise clip and volatility of the noise from FORK paper \n",
    "        self.noise_generator = OrnsteinUhlenbeckNoise(mu=np.zeros(self.act_dim), theta=kwargs.get(\"ou_noise_theta\", 4.0), sigma=kwargs.get(\"ou_noise_sigma\", 0.2), dt=kwargs.get(\"ou_dt\", 0.04))\n",
    "        self.target_noise = GaussianNoise(mu=np.zeros(self.act_dim), sigma=kwargs.get(\"noise_sigma\",0.1), clip=kwargs.get(\"noise_clip\", 0.5))\n",
    "        self.policy_noise = GaussianNoise(mu=np.zeros(self.act_dim), sigma=kwargs.get(\"noise_sigma\",0.2))\n",
    "        self.noise_comb = kwargs.get(\"noise_comb\", \"GOU\")\n",
    "        print(self.noise_comb)\n",
    "        \n",
    "        self.replay_memory_buffer = deque(maxlen = kwargs.get(\"buffer_capacity\", 1000000))\n",
    "        self.replay_memory_bufferd_dis = deque(maxlen = kwargs.get(\"buffer_capacity\", 1000000))\n",
    "\n",
    "        self.set_weights()\n",
    "        self.iteration = kwargs.get(\"iteration\", 0)\n",
    "\n",
    "    def load_weight(self):\n",
    "        self.actor.load_state_dict(torch.load(f'/weights/hardcore/{self.iteration}actor.pth', map_location=self.device))\n",
    "        self.critic.load_state_dict(torch.load(f'/weights/hardcore/{self.iteration}critic.pth', map_location=self.device))\n",
    "        self.actor_target.load_state_dict(torch.load(f'/weights/hardcore/{self.iteration}actor_t.pth', map_location=self.device))\n",
    "        self.critic_target.load_state_dict(torch.load(f'/weights/hardcore/{self.iteration}critic_t.pth', map_location=self.device))\n",
    "        self.system.load_state_dict(torch.load(f'/weights/hardcore/{self.iteration}sysmodel.pth', map_location=self.device))\n",
    "    \n",
    "    def set_weights(self):\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "\n",
    "    def add_to_replay_buffer(self, t, buffer):\n",
    "        buffer.append(t)\n",
    "\n",
    "    def sample_replay_buffer(self, buffer):\n",
    "        #  saple batch of observations from replay buffer\n",
    "        sample = random.sample(buffer, self.batch_size)\n",
    "        return sample\n",
    "\n",
    "    def learn(self, training_iterations, weight, train):\n",
    "        \"\"\"\n",
    "        training_iterations:Int -> how many times to train the networks\n",
    "        train:Bool -> should the networks be trained\n",
    "        weight:Float -> adaptive weight of the experience\n",
    "        \"\"\"\n",
    "        if len(self.replay_memory_buffer) < 1e4:\n",
    "            return 1\n",
    "\n",
    "        for it in range(training_iterations):\n",
    "            mini_batch = self.sample_replay_buffer(self.replay_memory_buffer)\n",
    "            state_batch = torch.from_numpy(np.vstack([i[0] for i in mini_batch])).float().to(self.device)\n",
    "            action_batch = torch.from_numpy(np.vstack([i[1] for i in mini_batch])).float().to(self.device)\n",
    "            reward_batch = torch.from_numpy(np.vstack([i[2] for i in mini_batch])).float().to(self.device)\n",
    "            add_reward_batch = torch.from_numpy(np.vstack([i[3] for i in mini_batch])).float().to(self.device)\n",
    "            next_state_batch = torch.from_numpy(np.vstack([i[4] for i in mini_batch])).float().to(self.device)\n",
    "            done_batch = torch.from_numpy(np.vstack([i[5] for i in mini_batch]).astype(np.uint8)).float().to(self.device)\n",
    "\n",
    "            #  train critic double q network\n",
    "            target_actions = self.actor_target(next_state_batch)\n",
    "            #  noise already clipped in the noise class\n",
    "            if self.noise_comb in [\"GOU\", \"GG\"]:\n",
    "                offset_noises = torch.from_numpy(self.target_noise()).float().to(self.device)\n",
    "            else:\n",
    "                offset_noises = torch.from_numpy(self.noise_generator()).float().to(self.device)\n",
    "                # 0.5 set as default clip should be added to hyperparameters later\n",
    "                offset_noises = offset_noises.clamp(-0.5, 0.5)\n",
    "            # below as in original implementation\n",
    "            target_actions = (target_actions + offset_noises).clamp(self.act_lower_bound, self.act_upper_bound)\n",
    "\n",
    "            Q_targets1, Q_targets2 = self.critic_target(next_state_batch, target_actions)\n",
    "            Q_targets = torch.min(Q_targets1, Q_targets2)\n",
    "            Q_targets = reward_batch + self.gamma * Q_targets * (1 - done_batch)\n",
    "\n",
    "            current_Q1, current_Q2 = self.critic(state_batch, action_batch)\n",
    "\n",
    "            critic_loss = F.mse_loss(current_Q1, Q_targets.detach()) + F.mse_loss(current_Q2, Q_targets.detach())\n",
    "\n",
    "            self.critic_optim.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optim.step()\n",
    "\n",
    "            self.soft_update_target(self.critic, self.critic_target)\n",
    "\n",
    "            #  predict new state with system network\n",
    "            predict_next_state = self.system(state_batch, action_batch) * (1-done_batch)\n",
    "            next_state_batch = next_state_batch * (1 -done_batch)\n",
    "            # train system network\n",
    "            system_loss = F.mse_loss(predict_next_state, next_state_batch.detach())\n",
    "\n",
    "            self.system_optim.zero_grad()\n",
    "            system_loss.backward()\n",
    "            self.system_optim.step()\n",
    "\n",
    "            s_flag = 1 if system_loss.item() < 0.020  else 0\n",
    "\n",
    "            if it % self.policy_freq == 0 and train == True:\n",
    "                actions = self.actor(state_batch)\n",
    "                actor_loss1,_ = self.critic_target(state_batch, actions)\n",
    "                actor_loss1 =  actor_loss1.mean()\n",
    "                actor_loss =  - actor_loss1 \n",
    "                if s_flag == 1:\n",
    "                    p_actions = self.actor(state_batch)\n",
    "                    p_next_state = self.system(state_batch, p_actions).clamp(self.obs_lower_bound,self.obs_upper_bound)\n",
    "\n",
    "                    p_actions2 = self.actor(p_next_state.detach()) * self.act_upper_bound\n",
    "                    actor_loss2,_ = self.critic_target(p_next_state.detach(), p_actions2)\n",
    "                    actor_loss2 = actor_loss2.mean() \n",
    "\n",
    "                    p_next_state2= self.system(p_next_state.detach(), p_actions2).clamp(self.obs_lower_bound,self.obs_upper_bound)\n",
    "                    p_actions3 = self.actor(p_next_state2.detach()) * self.act_upper_bound\n",
    "                    actor_loss3,_ = self.critic_target(p_next_state2.detach(), p_actions3)\n",
    "                    actor_loss3 = actor_loss3.mean() \n",
    "\n",
    "                    actor_loss_final =  actor_loss - weight * (actor_loss2) - 0.5 *  weight * actor_loss3\n",
    "                else:\n",
    "                    actor_loss_final =  actor_loss\n",
    "\n",
    "                self.actor_optim.zero_grad()\n",
    "                actor_loss_final.backward()\n",
    "                self.actor_optim.step()\n",
    "\n",
    "                self.soft_update_target(self.actor, self.actor_target)\n",
    "        return system_loss.item()\n",
    "\n",
    "    def soft_update_target(self,local_model,target_model):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(self.tau*local_param.data + (1.0-self.tau)*target_param.data)\n",
    "\n",
    "    def policy(self,state):\n",
    "        \"\"\"select action based on ACTOR\"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        self.actor.eval()\n",
    "        with torch.no_grad():\n",
    "            actions = self.actor(state).cpu().data.numpy()\n",
    "        self.actor.train()\n",
    "        # Adding noise to action\n",
    "        if self.noise_comb in [\"OUOU\", \"GOU\"]:\n",
    "            shift_action = self.noise_generator()\n",
    "        else:\n",
    "            shift_action = self.policy_noise()\n",
    "        sampled_actions = (actions + shift_action)\n",
    "        # We make sure action is within bounds\n",
    "        legal_action = np.clip(sampled_actions,self.act_lower_bound,self.act_upper_bound)\n",
    "        return np.squeeze(legal_action)\n",
    "\n",
    "    def select_action(self,state):\n",
    "        \"\"\"select action based on ACTOR\"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            actions = self.actor_target(state).cpu().data.numpy()\n",
    "        return np.squeeze(actions)\n",
    "\n",
    "\n",
    "    def eval_policy(self, env_name, seed, eval_episodes):\n",
    "        eval_env = env_name\n",
    "        eval_env.seed(seed + 100)\n",
    "\n",
    "        avg_reward = 0.\n",
    "        for _ in range(eval_episodes):\n",
    "            state, done = eval_env.reset(), False\n",
    "            while not done:\n",
    "                action = self.select_action(np.array(state))\n",
    "                state, reward, done, info = eval_env.step(action)\n",
    "                try:\n",
    "                    avg_reward += info[\"reward\"]\n",
    "                except:\n",
    "                    avg_reward += reward\n",
    "        avg_reward /= eval_episodes\n",
    "\n",
    "        print(\"---------------------------------------\")\n",
    "        print(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
    "        print(\"---------------------------------------\")\n",
    "        return avg_reward\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_dirs(meta_f, env_type, model_type, dirs):\n",
    "    iteration_number = 0\n",
    "    paths = {}\n",
    "    if not os.path.exists(meta_f):\n",
    "        with open(meta_f, \"w\") as file:\n",
    "            file.write(str(iteration_number))\n",
    "    with open(meta_f, \"r\") as file:\n",
    "        try:\n",
    "            iteration_number = int(file.read())\n",
    "        except:\n",
    "            pass\n",
    "    iteration_number += 1\n",
    "    with open(meta_f, \"w\") as file:\n",
    "        file.write(str(iteration_number))\n",
    "    for d in dirs:\n",
    "        os.makedirs(f'{d}/{env_type}/{model_type}/{iteration_number}', exist_ok=True)\n",
    "        paths[d] = f'{d}/{env_type}/{model_type}/{iteration_number}'\n",
    "    return paths,iteration_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 6250, timestep 5271947,  ep.timesteps 795, reward: 315.51, moving avg. reward: 315.06, time: 02:29:26\n",
      "episode 6251, timestep 5272750,  ep.timesteps 803, reward: 315.16, moving avg. reward: 315.06, time: 02:29:27\n",
      "episode 6252, timestep 5273553,  ep.timesteps 803, reward: 314.50, moving avg. reward: 315.06, time: 02:29:28\n",
      "episode 6253, timestep 5274348,  ep.timesteps 795, reward: 314.27, moving avg. reward: 315.05, time: 02:29:28\n",
      "episode 6254, timestep 5275145,  ep.timesteps 797, reward: 315.38, moving avg. reward: 315.06, time: 02:29:29\n",
      "episode 6255, timestep 5275949,  ep.timesteps 804, reward: 314.34, moving avg. reward: 315.05, time: 02:29:30\n",
      "episode 6256, timestep 5276738,  ep.timesteps 789, reward: 315.06, moving avg. reward: 315.06, time: 02:29:30\n",
      "episode 6257, timestep 5277533,  ep.timesteps 795, reward: 316.04, moving avg. reward: 315.07, time: 02:29:31\n",
      "episode 6258, timestep 5278323,  ep.timesteps 790, reward: 315.47, moving avg. reward: 315.08, time: 02:29:32\n",
      "episode 6259, timestep 5279129,  ep.timesteps 806, reward: 314.47, moving avg. reward: 315.07, time: 02:29:33\n",
      "episode 6260, timestep 5279922,  ep.timesteps 793, reward: 315.26, moving avg. reward: 315.07, time: 02:29:34\n",
      "episode 6261, timestep 5280721,  ep.timesteps 799, reward: 315.02, moving avg. reward: 315.07, time: 02:29:34\n",
      "episode 6262, timestep 5281502,  ep.timesteps 781, reward: 315.55, moving avg. reward: 315.08, time: 02:29:35\n",
      "episode 6263, timestep 5282288,  ep.timesteps 786, reward: 315.50, moving avg. reward: 315.09, time: 02:29:36\n",
      "episode 6264, timestep 5283086,  ep.timesteps 798, reward: 315.64, moving avg. reward: 315.09, time: 02:29:37\n",
      "episode 6265, timestep 5283885,  ep.timesteps 799, reward: 315.33, moving avg. reward: 315.09, time: 02:29:37\n",
      "episode 6266, timestep 5284684,  ep.timesteps 799, reward: 314.70, moving avg. reward: 315.09, time: 02:29:38\n",
      "episode 6267, timestep 5285477,  ep.timesteps 793, reward: 315.39, moving avg. reward: 315.10, time: 02:29:39\n",
      "episode 6268, timestep 5286271,  ep.timesteps 794, reward: 315.32, moving avg. reward: 315.10, time: 02:29:40\n",
      "episode 6269, timestep 5287070,  ep.timesteps 799, reward: 314.72, moving avg. reward: 315.09, time: 02:29:40\n",
      "episode 6270, timestep 5287862,  ep.timesteps 792, reward: 314.56, moving avg. reward: 315.08, time: 02:29:41\n",
      "episode 6271, timestep 5288662,  ep.timesteps 800, reward: 314.80, moving avg. reward: 315.07, time: 02:29:42\n",
      "episode 6272, timestep 5289458,  ep.timesteps 796, reward: 315.26, moving avg. reward: 315.07, time: 02:29:43\n",
      "episode 6273, timestep 5290272,  ep.timesteps 814, reward: 314.05, moving avg. reward: 315.07, time: 02:29:43\n",
      "episode 6274, timestep 5291058,  ep.timesteps 786, reward: 315.12, moving avg. reward: 315.08, time: 02:29:44\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 85\u001b[0m\n\u001b[1;32m     82\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mpolicy(state)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Recieve state and reward from environment.\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m next_state, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m#change original reward from -100 to -5 and 5*reward for other values\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrapper:\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/gym/wrappers/monitor.py:49\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_before_step\u001b[49m(action)\n\u001b[1;32m     50\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     51\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_step(observation, reward, done, info)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#init storage files for easy organisation during experimentation\n",
    "log_dir = \"./logs\"\n",
    "plot_dir = \"./plots\"\n",
    "video_dir = \"./videos\"\n",
    "models_dir = \"./models\"\n",
    "\n",
    "meta_f = \"./metadata.txt\"\n",
    "logs_f = \"/agent-log.txt\"\n",
    "\n",
    "env_type = \"normal\"\n",
    "model_type = \"td3-fork\"\n",
    "wrapper = False\n",
    "\n",
    "_paths, iteration_number = initialise_dirs(meta_f, env_type, model_type, [log_dir, plot_dir, video_dir, models_dir])\n",
    "\n",
    "if env_type != \"hardcore\":\n",
    "    env = gym.make('BipedalWalker-v3')\n",
    "else:\n",
    "    env = gym.make('BipedalWalkerHardcore-v3')\n",
    "\n",
    "plot_every = 25\n",
    "video_every = 25\n",
    "\n",
    "env = gym.wrappers.Monitor(env, _paths['./videos'], video_callable=lambda ep_id: ep_id%video_every == 0, force=True)\n",
    "if wrapper:\n",
    "    env = EnvSkipWrapper(env)\n",
    "with open(_paths['./logs'] + logs_f, \"w\") as file:\n",
    "        try:\n",
    "            pass\n",
    "        except:\n",
    "            pass\n",
    "log_f = open(_paths['./logs'] + logs_f, \"w\")\n",
    "\n",
    "exploration_steps = 0\n",
    "total_episodes = 10000\n",
    "max_steps = 2000\n",
    "total_steps =0\n",
    "add_experience_count = 0\n",
    "save_every = 1000\n",
    "train = 0\n",
    "saved_times = 0\n",
    "time_start = time.time() \n",
    "totest =0\n",
    "\n",
    "ep_reward_list = []\n",
    "avg_reward_list = []\n",
    "data = []\n",
    "agent = Agent(env,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    seed=42,\n",
    "    load_weight=False,\n",
    "    lr_critic=3e-4,\n",
    "    lr_actor=3e-4,\n",
    "    lr_system=3e-4,\n",
    "    batch_size=100,\n",
    "    gamma=0.99,\n",
    "    tau=0.02,\n",
    "    policy_freq=2,\n",
    "    ou_noise_theta=4,\n",
    "    ou_noise_sigma=0.2,\n",
    "    ou_dt=0.04,\n",
    "    noise_sigma=0.1,\n",
    "    noise_clip=0.5,\n",
    "    noise_comb=\"GG\",\n",
    "    buffer_capacity=1000000,\n",
    "    iteration=iteration_number\n",
    ")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for ep in range(total_episodes):\n",
    "        state = env.reset()\n",
    "        ep_reward = 0\n",
    "        t = int(0)\n",
    "        temp_replay_buffer = []\n",
    "\n",
    "        for st in range(max_steps):\n",
    "\n",
    "            # Select action randomly or according to policy\n",
    "            if total_steps < exploration_steps:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = agent.policy(state)\n",
    "\n",
    "            # Recieve state and reward from environment.\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            #change original reward from -100 to -5 and 5*reward for other values\n",
    "            if wrapper:\n",
    "                ep_reward += info[\"reward\"]\n",
    "                if info[\"dead\"]:\n",
    "                    add_reward = -1\n",
    "                    add_experience_count += 1\n",
    "                else:\n",
    "                    add_reward = 0\n",
    "            else:\n",
    "                ep_reward += reward\n",
    "                if reward == -100:\n",
    "                    add_reward = -1\n",
    "                    reward = -5\n",
    "                    add_experience_count += 1\n",
    "                else:\n",
    "                    add_reward = 0\n",
    "                    reward = 5 * reward\n",
    "\n",
    "            temp_replay_buffer.append((state, action, reward, add_reward, next_state, done))\n",
    "            \n",
    "            # End this episode when `done` is True\n",
    "            if done:\n",
    "                if add_reward == -1 or ep_reward < 250:            \n",
    "                    train = 1\n",
    "                    for temp in temp_replay_buffer: \n",
    "                        agent.add_to_replay_buffer(temp, agent.replay_memory_buffer)\n",
    "                elif add_experience_count > 0 and np.random.rand() > 0.5:\n",
    "                    train = 1\n",
    "                    add_experience_count -= 10\n",
    "                    for temp in temp_replay_buffer: \n",
    "                        agent.add_to_replay_buffer(temp, agent.replay_memory_buffer)\n",
    "                break\n",
    "            state = next_state\n",
    "            t += int(1)\n",
    "            total_steps += 1\n",
    "            # agent.step_end() # let decaying ou know the end of the step\n",
    "        # if ep_reward > 150:\n",
    "        #     print(info[\"c\"])\n",
    "        ep_reward_list.append(ep_reward)\n",
    "        # Mean of last 100 episodes\n",
    "        avg_reward = np.mean(ep_reward_list[-100:])\n",
    "        avg_reward_list.append(avg_reward)\n",
    "        # agent.episode_end()\n",
    "\n",
    "        if np.mean(ep_reward_list[-5:]) > 300 and totest == 0:\n",
    "            test_reward = agent.eval_policy(env, seed=42, eval_episodes=10)\n",
    "            if test_reward > 300:\n",
    "                final_test_reward = agent.eval_policy(env, seed=42, eval_episodes=100)\n",
    "                if final_test_reward > 300:\n",
    "                  \n",
    "                    torch.save(agent.actor.state_dict(), f'{_paths[\"./models\"]}/final-actor.pth')\n",
    "                    torch.save(agent.critic.state_dict(), f'{_paths[\"./models\"]}/final-critic.pth')\n",
    "                    torch.save(agent.actor_target.state_dict(), f'{_paths[\"./models\"]}/final-actor_t.pth')\n",
    "                    torch.save(agent.critic_target.state_dict(), f'{_paths[\"./models\"]}/final-critic_t.pth')\n",
    "                    torch.save(agent.system.state_dict(), f'{_paths[\"./models\"]}/final-sysmodel.pth')\n",
    "\n",
    "                    print(\"===========================\")\n",
    "                    print('Task Solved')\n",
    "                    print(\"===========================\")\n",
    "                    # break\n",
    "                    totest = 1\n",
    "                    \n",
    "        s = (int)(time.time() - time_start)\n",
    "\n",
    "       \n",
    "        #Training agent only when new experiences are added to the replay buffer\n",
    "        weight =  1 - np.clip(np.mean(ep_reward_list[-100:])/300, 0, 1)\n",
    "        if train == 1:\n",
    "            sys_loss = agent.learn(t, weight, train)\n",
    "        else: \n",
    "            sys_loss = agent.learn(100, weight, train)\n",
    "        train = 0\n",
    "\n",
    "            # do NOT change this logging code - it is used for automated marking!\n",
    "        log_f.write('episode: {}, reward: {}\\n'.format(ep, ep_reward))\n",
    "        log_f.flush()\n",
    "\n",
    "        if ep % plot_every == 0:\n",
    "            data.append([ep, np.array(ep_reward_list[-100:]).mean(), np.array(ep_reward_list[-100:]).std(), t])\n",
    "            # ep_reward_list = []\n",
    "            # plt.rcParams['figure.dpi'] = 100\n",
    "            plt.plot([x[0] for x in data], [x[1] for x in data], '-', color='tab:grey')\n",
    "            plt.fill_between([x[0] for x in data], [x[1]-x[2] for x in data], [x[1]+x[2] for x in data], alpha=0.2, color='tab:grey')\n",
    "            plt.xlabel('Episode number')\n",
    "            plt.ylabel('Episode reward')\n",
    "            plt.savefig(_paths['./plots']+'/plot_ep_{}.png'.format(ep))\n",
    "            plt.show()\n",
    "            disp.clear_output(wait=True)\n",
    "\n",
    "        print('episode {}, timestep {},  ep.timesteps {}, reward: {:.2f}, moving avg. reward: {:.2f}, time: {:02}:{:02}:{:02}'\n",
    "                .format(ep, total_steps, t,\n",
    "                      ep_reward, avg_reward, s//3600, s%3600//60, s%60)) \n",
    "        ep_reward = 0   \n",
    "        \n",
    "        if t % 500 == 0:           \n",
    "            \n",
    "            torch.save(agent.actor.state_dict(), f'{_paths[\"./models\"]}/{saved_times}-actor.pth')\n",
    "            torch.save(agent.critic.state_dict(), f'{_paths[\"./models\"]}/{saved_times}-critic.pth')\n",
    "            torch.save(agent.actor_target.state_dict(), f'{_paths[\"./models\"]}/{saved_times}-actor_t.pth')\n",
    "            torch.save(agent.critic_target.state_dict(), f'{_paths[\"./models\"]}/{saved_times}-critic_t.pth')\n",
    "            torch.save(agent.system.state_dict(), f'{_paths[\"./models\"]}/{saved_times}-sysmodel.pth')        \n",
    "            print(\"===========================\")\n",
    "            print('Saving Successfully!')\n",
    "            print(\"===========================\")\n",
    "            saved_times +=1\n",
    "        \n",
    "      \n",
    "\n",
    "# Plotting graph\n",
    "# Episodes versus Avg. Rewards\n",
    "plt.plot(avg_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
    "plt.show()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
